{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom function for print a sentence\n",
    "def goodPrint(a):\n",
    "    x = \"\"\n",
    "    for i in range(len(a)):\n",
    "        x += a[i]+\" \"\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading nltk corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cess_esp to /home/carban/nltk_data...\n",
      "[nltk_data]   Package cess_esp is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('cess_esp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import cess_esp\n",
    "\n",
    "tagged_sentences = cess_esp.tagged_sents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We get a corpus with sentences and the tag for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'El', u'da0ms0'), (u'grupo', u'ncms000'), (u'estatal', u'aq0cs0'), (u'Electricit\\xe9_de_France', u'np00000'), (u'-Fpa-', u'Fpa'), (u'EDF', u'np00000'), (u'-Fpt-', u'Fpt'), (u'anunci\\xf3', u'vmis3s0'), (u'hoy', u'rg'), (u',', u'Fc'), (u'jueves', u'W'), (u',', u'Fc'), (u'la', u'da0fs0'), (u'compra', u'ncfs000'), (u'del', u'spcms'), (u'51_por_ciento', u'Zp'), (u'de', u'sps00'), (u'la', u'da0fs0'), (u'empresa', u'ncfs000'), (u'mexicana', u'aq0fs0'), (u'Electricidad_\\xc1guila_de_Altamira', u'np00000'), (u'-Fpa-', u'Fpa'), (u'EAA', u'np00000'), (u'-Fpt-', u'Fpt'), (u',', u'Fc'), (u'creada', u'aq0fsp'), (u'por', u'sps00'), (u'el', u'da0ms0'), (u'japon\\xe9s', u'aq0ms0'), (u'Mitsubishi_Corporation', u'np00000'), (u'para', u'sps00'), (u'poner_en_marcha', u'vmn0000'), (u'una', u'di0fs0'), (u'central', u'ncfs000'), (u'de', u'sps00'), (u'gas', u'ncms000'), (u'de', u'sps00'), (u'495', u'Z'), (u'megavatios', u'ncmp000'), (u'.', u'Fp')]\n",
      "==========================\n",
      "('Tagged sentences: ', 6030)\n",
      "('Tagged words:', 192685)\n"
     ]
    }
   ],
   "source": [
    "print(tagged_sentences[0])\n",
    "print(\"==========================\")\n",
    "print(\"Tagged sentences: \", len(tagged_sentences))\n",
    "print(\"Tagged words:\", len(cess_esp.tagged_words()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Sentence and tag inside two different arrays, later write 2 files for the 2 arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "sentences, tagss = [], []\n",
    "for tagged_sentence in tagged_sentences:\n",
    "    sentence, tags = zip(*tagged_sentence)\n",
    "    sentences.append(np.array(sentence))\n",
    "    tagss.append(np.array(tags))\n",
    "    \n",
    "# with open(\"sentences.txt\", \"wb\") as fp:\n",
    "#     pickle.dump(sentences, fp)\n",
    "    \n",
    "# with open(\"tags.txt\", \"wb\") as fp:\n",
    "#     pickle.dump(tagss, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing a sample of the arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6030\n",
      "\n",
      "Words on the first sentence: 40\n",
      "\n",
      "El grupo estatal Electricité_de_France -Fpa- EDF -Fpt- anunció hoy , jueves , la compra del 51_por_ciento de la empresa mexicana Electricidad_Águila_de_Altamira -Fpa- EAA -Fpt- , creada por el japonés Mitsubishi_Corporation para poner_en_marcha una central de gas de 495 megavatios . \n",
      "\n",
      "tags of the first sentence: 40\n",
      "\n",
      "[u'da0ms0' u'ncms000' u'aq0cs0' u'np00000' u'Fpa' u'np00000' u'Fpt'\n",
      " u'vmis3s0' u'rg' u'Fc' u'W' u'Fc' u'da0fs0' u'ncfs000' u'spcms' u'Zp'\n",
      " u'sps00' u'da0fs0' u'ncfs000' u'aq0fs0' u'np00000' u'Fpa' u'np00000'\n",
      " u'Fpt' u'Fc' u'aq0fsp' u'sps00' u'da0ms0' u'aq0ms0' u'np00000' u'sps00'\n",
      " u'vmn0000' u'di0fs0' u'ncfs000' u'sps00' u'ncms000' u'sps00' u'Z'\n",
      " u'ncmp000' u'Fp']\n"
     ]
    }
   ],
   "source": [
    "print(str(len(sentences)) + \"\\n\")\n",
    "print(\"Words on the first sentence: \"+str(len(sentences[0])))+\"\\n\"\n",
    "print(goodPrint(sentences[0])+ \"\\n\")\n",
    "print(\"tags of the first sentence: \"+str(len(tagss[0])))+\"\\n\"\n",
    "print(tagss[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Percentages for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "(training_sentences, \n",
    " test_sentences, \n",
    " training_tags, \n",
    " test_tags) = train_test_split(sentences, tagss, test_size=0.2)\n",
    "\n",
    "(train_sentences, \n",
    " eval_sentences, \n",
    " train_tags, \n",
    " eval_tags) = train_test_split(training_sentences, training_tags, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_sentences:4824\n",
      "train_sentences: 3618\n",
      "test_sentences: 1206\n",
      "eval_sentences: 1206\n",
      "\n",
      "[u'Los' u'dos' u'mejores' u'jugadores' u'del' u'mundo' u'tuvieron' u'que'\n",
      " u'emplear' u'muchos' u'minutos' u'en' u'cada' u'jugada' u'dada' u'la'\n",
      " u'complejidad' u'de' u'la' u'posici\\xf3n' u'.']\n",
      "[u'Y' u'se' u'dec\\xeda' u',' u'en' u'el' u'murmulleo' u'madrile\\xf1o' u','\n",
      " u'que' u'otro' u't\\xedtulo' u'posterior' u'de' u'Juan' u',' u'Sub_rosa'\n",
      " u',' u'era' u'una' u'clara' u'alusi\\xf3n' u'a' u'ella' u'.']\n",
      "[u'\"' u'*0*' u'No' u'es' u'falta' u'de' u'informaci\\xf3n' u',' u'*0*'\n",
      " u'es' u'ocultaci\\xf3n' u'de' u'informaci\\xf3n' u'\"' u',' u'dijo'\n",
      " u'Zenarruzabeitia' u',' u'para' u'quien' u'la' u'comparecencia' u'de'\n",
      " u'ayer' u'de' u'Mayor_Oreja' u'\"' u'no' u'fue' u'de' u'recibo' u'\"'\n",
      " u'porque' u'en_vez_de' u'\"' u'responder' u'detalladamente' u'\"' u'a'\n",
      " u'la' u'interpelaci\\xf3n' u'del' u'PNV' u'*0*' u'se' u'dedic\\xf3' u'a'\n",
      " u'\"' u'la' u'descalificaci\\xf3n' u'de' u'un' u'grupo' u'pol\\xedtico' u'y'\n",
      " u'a' u'hablar' u'de' u'cuestiones' u'que' u'no' u'tienen' u'nada' u'que'\n",
      " u'ver' u',' u'*0*' u'utiliz\\xf3' u'la' u'pol\\xedtica' u'del'\n",
      " u'chipir\\xf3n' u'de' u'responder' u'atacando' u'\"' u'.']\n",
      "\n",
      "training_tags:4824\n",
      "train_tags: 3618\n",
      "test_tags: 1206\n",
      "eval_tags: 1206\n",
      "\n",
      "[u'da0mp0' u'dn0cp0' u'aq0cp0' u'ncmp000' u'spcms' u'ncms000' u'vmis3p0'\n",
      " u'cs' u'vmn0000' u'di0mp0' u'ncmp000' u'sps00' u'di0cs0' u'ncfs000'\n",
      " u'vmp00sf' u'da0fs0' u'ncfs000' u'sps00' u'da0fs0' u'ncfs000' u'Fp']\n",
      "[u'cc' u'p0000000' u'vmii3s0' u'Fc' u'sps00' u'da0ms0' u'ncms000'\n",
      " u'aq0ms0' u'Fc' u'cs' u'di0ms0' u'ncms000' u'aq0cs0' u'sps00' u'np0000p'\n",
      " u'Fc' u'np0000a' u'Fc' u'vsii3s0' u'di0fs0' u'aq0fs0' u'ncfs000' u'sps00'\n",
      " u'pp3fs000' u'Fp']\n",
      "[u'Fe' u'sn.e-SUJ' u'rn' u'vsip3s0' u'ncfs000' u'sps00' u'ncfs000' u'Fc'\n",
      " u'sn.e-SUJ' u'vsip3s0' u'ncfs000' u'sps00' u'ncfs000' u'Fe' u'Fc'\n",
      " u'vmis3s0' u'np0000p' u'Fc' u'sps00' u'pr0cs000' u'da0fs0' u'ncfs000'\n",
      " u'sps00' u'rg' u'sps00' u'np0000p' u'Fe' u'rn' u'vsis3s0' u'sps00'\n",
      " u'ncms000' u'Fe' u'cs' u'sps00' u'Fe' u'vmn0000' u'rg' u'Fe' u'sps00'\n",
      " u'da0fs0' u'ncfs000' u'spcms' u'np0000o' u'sn.e-SUJ' u'p0300000'\n",
      " u'vmis3s0' u'sps00' u'Fe' u'da0fs0' u'ncfs000' u'sps00' u'di0ms0'\n",
      " u'ncms000' u'aq0ms0' u'cc' u'sps00' u'vmn0000' u'sps00' u'ncfp000'\n",
      " u'pr0cn000' u'rn' u'vmip3p0' u'pi0cs000' u'pr0cn000' u'vmn0000' u'Fc'\n",
      " u'sn.e-SUJ' u'vmis3s0' u'da0fs0' u'ncfs000' u'spcms' u'ncms000' u'sps00'\n",
      " u'vmn0000' u'vmg0000' u'Fe' u'Fp']\n"
     ]
    }
   ],
   "source": [
    "print(\"training_sentences:\" + str(len(training_sentences)))\n",
    "print(\"train_sentences: \" + str(len(train_sentences)))\n",
    "print(\"test_sentences: \" + str(len(test_sentences)))\n",
    "print(\"eval_sentences: \" + str(len(eval_sentences)) + \"\\n\")\n",
    "\n",
    "print(train_sentences[0])\n",
    "print(test_sentences[0])\n",
    "print(eval_sentences[0])\n",
    "\n",
    "print(\"\\ntraining_tags:\" + str(len(training_sentences)))\n",
    "print(\"train_tags: \" + str(len(train_tags)))\n",
    "print(\"test_tags: \" + str(len(test_tags)))\n",
    "print(\"eval_tags: \" + str(len(eval_tags)) + \"\\n\")\n",
    "\n",
    "print(train_tags[0])\n",
    "print(test_tags[0])\n",
    "print(eval_tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24499\n",
      "291\n"
     ]
    }
   ],
   "source": [
    "words, tagsss = set([]), set([])\n",
    " \n",
    "for s in (train_sentences + eval_sentences + test_sentences):\n",
    "    for w in s:\n",
    "        words.add(w.lower())\n",
    "\n",
    "for ts in (train_tags + eval_tags + test_tags):\n",
    "    for t in ts:\n",
    "        tagsss.add(t)\n",
    "\n",
    "word2index = {w: i + 2 for i, w in enumerate(list(words))}\n",
    "word2index['-PAD-'] = 0  # The special value used for padding\n",
    "word2index['-OOV-'] = 1  # The special value used for OOVs\n",
    " \n",
    "tag2index = {t: i + 2 for i, t in enumerate(list(tagsss))}\n",
    "tag2index['-PAD-'] = 0  # The special value used to padding\n",
    "tag2index['-OOV-'] = 1  # The special value used to padding\n",
    "\n",
    "print (len(word2index))\n",
    "print (len(tag2index))\n",
    "\n",
    "# print(word2index)\n",
    "# print(tag2index)\n",
    "\n",
    "np.save('word2index.npy', word2index)\n",
    "np.save('tag2index.npy', tag2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences_X, eval_sentences_X, test_sentences_X, train_tags_y, eval_tags_y, test_tags_y = [], [], [], [], [], []\n",
    "\n",
    "for s in train_sentences:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    " \n",
    "    train_sentences_X.append(s_int)\n",
    "\n",
    "for s in eval_sentences:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    " \n",
    "    eval_sentences_X.append(s_int)\n",
    "\n",
    "for s in test_sentences:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    " \n",
    "    test_sentences_X.append(s_int)\n",
    "\n",
    "for s in train_tags:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(tag2index[w])\n",
    "        except KeyError:\n",
    "            s_int.append(tag2index['-OOV-'])\n",
    "            \n",
    "    train_tags_y.append(s_int)\n",
    "\n",
    "for s in eval_tags:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(tag2index[w])\n",
    "        except KeyError:\n",
    "            s_int.append(tag2index['-OOV-'])\n",
    "            \n",
    "    eval_tags_y.append(s_int)\n",
    "\n",
    "for s in test_tags:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(tag2index[w])\n",
    "        except KeyError:\n",
    "            s_int.append(tag2index['-OOV-'])\n",
    "            \n",
    "    test_tags_y.append(s_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitudes de las Matrices:\n",
      "3618\n",
      "1206\n",
      "1206\n",
      "3618\n",
      "1206\n",
      "1206\n",
      "\n",
      "Muestra de Datos presentes en las Matrices con las transformaciones:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"Longitudes de las Matrices:\")\n",
    "print(len(train_sentences_X))\n",
    "print(len(eval_sentences_X))\n",
    "print(len(test_sentences_X))\n",
    "print(len(train_tags_y))\n",
    "print(len(eval_tags_y))\n",
    "print(len(test_tags_y))\n",
    "\n",
    "print(\"\\nMuestra de Datos presentes en las Matrices con las transformaciones:\\n\")\n",
    "\n",
    "# print(train_sentences_X[0])\n",
    "# print(eval_sentences_X[0])\n",
    "# print(test_sentences_X[0])\n",
    "# print(train_tags_y[0])\n",
    "# print(eval_tags_y[0])\n",
    "# print(test_tags_y[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words on the first sentence: 21\n",
      "\n",
      "Los dos mejores jugadores del mundo tuvieron que emplear muchos minutos en cada jugada dada la complejidad de la posición . \n",
      "\n",
      "Words on the first sentence PARSED: 21\n",
      "\n",
      "[14514, 19603, 19702, 8312, 10028, 722, 16732, 4366, 16793, 8692, 16154, 8174, 22415, 7924, 4895, 10462, 21317, 7865, 10462, 18359, 241]\n",
      "\n",
      "\n",
      "tags of the first sentence: 21\n",
      "\n",
      "[u'da0mp0' u'dn0cp0' u'aq0cp0' u'ncmp000' u'spcms' u'ncms000' u'vmis3p0'\n",
      " u'cs' u'vmn0000' u'di0mp0' u'ncmp000' u'sps00' u'di0cs0' u'ncfs000'\n",
      " u'vmp00sf' u'da0fs0' u'ncfs000' u'sps00' u'da0fs0' u'ncfs000' u'Fp']\n",
      "\n",
      "\n",
      "tags of the first sentence PARSED: 21\n",
      "\n",
      "[281, 134, 125, 34, 197, 252, 40, 156, 87, 138, 34, 262, 152, 199, 194, 215, 199, 262, 215, 199, 160]\n"
     ]
    }
   ],
   "source": [
    "print(\"Words on the first sentence: \"+str(len(train_sentences[0])))+\"\\n\"\n",
    "print(goodPrint(train_sentences[0])+ \"\\n\")\n",
    "print(\"Words on the first sentence PARSED: \"+str(len(train_sentences_X[0])))+\"\\n\"\n",
    "print(train_sentences_X[0])\n",
    "print(\"\\n\")\n",
    "print(\"tags of the first sentence: \"+str(len(train_tags[0])))+\"\\n\"\n",
    "print(train_tags[0])\n",
    "print(\"\\n\")\n",
    "print(\"tags of the first sentence PARSED: \"+str(len(train_tags_y[0])))+\"\\n\"\n",
    "print(train_tags_y[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
