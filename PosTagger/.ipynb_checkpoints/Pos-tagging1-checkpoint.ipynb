{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom function for print a sentence\n",
    "def goodPrint(a):\n",
    "    x = \"\"\n",
    "    for i in range(len(a)):\n",
    "        x += a[i]+\" \"\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading nltk corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cess_esp to /home/carban/nltk_data...\n",
      "[nltk_data]   Package cess_esp is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('cess_esp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import cess_esp\n",
    "\n",
    "tagged_sentences = cess_esp.tagged_sents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We get a corpus with sentences and the tag for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'El', u'da0ms0'), (u'grupo', u'ncms000'), (u'estatal', u'aq0cs0'), (u'Electricit\\xe9_de_France', u'np00000'), (u'-Fpa-', u'Fpa'), (u'EDF', u'np00000'), (u'-Fpt-', u'Fpt'), (u'anunci\\xf3', u'vmis3s0'), (u'hoy', u'rg'), (u',', u'Fc'), (u'jueves', u'W'), (u',', u'Fc'), (u'la', u'da0fs0'), (u'compra', u'ncfs000'), (u'del', u'spcms'), (u'51_por_ciento', u'Zp'), (u'de', u'sps00'), (u'la', u'da0fs0'), (u'empresa', u'ncfs000'), (u'mexicana', u'aq0fs0'), (u'Electricidad_\\xc1guila_de_Altamira', u'np00000'), (u'-Fpa-', u'Fpa'), (u'EAA', u'np00000'), (u'-Fpt-', u'Fpt'), (u',', u'Fc'), (u'creada', u'aq0fsp'), (u'por', u'sps00'), (u'el', u'da0ms0'), (u'japon\\xe9s', u'aq0ms0'), (u'Mitsubishi_Corporation', u'np00000'), (u'para', u'sps00'), (u'poner_en_marcha', u'vmn0000'), (u'una', u'di0fs0'), (u'central', u'ncfs000'), (u'de', u'sps00'), (u'gas', u'ncms000'), (u'de', u'sps00'), (u'495', u'Z'), (u'megavatios', u'ncmp000'), (u'.', u'Fp')]\n",
      "==========================\n",
      "('Tagged sentences: ', 6030)\n",
      "('Tagged words:', 192685)\n"
     ]
    }
   ],
   "source": [
    "print(tagged_sentences[0])\n",
    "print(\"==========================\")\n",
    "print(\"Tagged sentences: \", len(tagged_sentences))\n",
    "print(\"Tagged words:\", len(cess_esp.tagged_words()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Sentence and tag inside two different arrays, later write 2 files for the 2 arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "sentences, tagss = [], []\n",
    "for tagged_sentence in tagged_sentences:\n",
    "    sentence, tags = zip(*tagged_sentence)\n",
    "    sentences.append(np.array(sentence))\n",
    "    tagss.append(np.array(tags))\n",
    "    \n",
    "with open(\"sentences.txt\", \"wb\") as fp:\n",
    "    pickle.dump(sentences, fp)\n",
    "    \n",
    "with open(\"tags.txt\", \"wb\") as fp:\n",
    "    pickle.dump(tagss, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing a sample of the arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6030\n",
      "\n",
      "Words on the first sentence: 40\n",
      "\n",
      "El grupo estatal Electricité_de_France -Fpa- EDF -Fpt- anunció hoy , jueves , la compra del 51_por_ciento de la empresa mexicana Electricidad_Águila_de_Altamira -Fpa- EAA -Fpt- , creada por el japonés Mitsubishi_Corporation para poner_en_marcha una central de gas de 495 megavatios . \n",
      "\n",
      "tags of the first sentence: 40\n",
      "\n",
      "[u'da0ms0' u'ncms000' u'aq0cs0' u'np00000' u'Fpa' u'np00000' u'Fpt'\n",
      " u'vmis3s0' u'rg' u'Fc' u'W' u'Fc' u'da0fs0' u'ncfs000' u'spcms' u'Zp'\n",
      " u'sps00' u'da0fs0' u'ncfs000' u'aq0fs0' u'np00000' u'Fpa' u'np00000'\n",
      " u'Fpt' u'Fc' u'aq0fsp' u'sps00' u'da0ms0' u'aq0ms0' u'np00000' u'sps00'\n",
      " u'vmn0000' u'di0fs0' u'ncfs000' u'sps00' u'ncms000' u'sps00' u'Z'\n",
      " u'ncmp000' u'Fp']\n"
     ]
    }
   ],
   "source": [
    "print(str(len(sentences)) + \"\\n\")\n",
    "print(\"Words on the first sentence: \"+str(len(sentences[0])))+\"\\n\"\n",
    "print(goodPrint(sentences[0])+ \"\\n\")\n",
    "print(\"tags of the first sentence: \"+str(len(tagss[0])))+\"\\n\"\n",
    "print(tagss[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Percentages for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "(training_sentences, \n",
    " test_sentences, \n",
    " training_tags, \n",
    " test_tags) = train_test_split(sentences, tagss, test_size=0.2)\n",
    "\n",
    "(train_sentences, \n",
    " eval_sentences, \n",
    " train_tags, \n",
    " eval_tags) = train_test_split(training_sentences, training_tags, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_sentences:4824\n",
      "train_sentences: 3618\n",
      "test_sentences: 1206\n",
      "eval_sentences: 1206\n",
      "\n",
      "[u'*0*' u'No' u'se' u'ven' u'las' u'caras' u'desde' u\"Atlanta'96\" u'y'\n",
      " u'aunque' u'Freeman' u'tiene' u'todos' u'los' u'pronunciamientos' u'a'\n",
      " u'su' u'favor' u',' u'puesto_que' u'*0*' u'se' u'ha' u'mostrado'\n",
      " u'intratable' u'en' u'los' u'\\xfaltimos' u'a\\xf1os' u',' u'no' u'se'\n",
      " u'puede' u'descartar' u'a' u'toda' u'una' u'doble' u'campeona'\n",
      " u'ol\\xedmpica' u'en' u'200' u'y' u'400' u'como' u'la' u'francesa' u'.']\n",
      "[u'El' u'investigador' u'que' u'nunca' u'descubre' u'nada' u'que' u'no'\n",
      " u'se' u'populariza' u'gracias_a' u'alg\\xfan' u'descubrimiento'\n",
      " u'suficientemente' u'espectacular' u'que' u'lo' u'haga' u'aparecer' u'en'\n",
      " u'alg\\xfan' u'magazine' u'televisivo' u',' u'va' u'perdiendo' u'las'\n",
      " u'fuentes' u'de' u'financiaci\\xf3n' u'.']\n",
      "[u'La' u'nadadora' u'parapl\\xe9jica' u'espa\\xf1ola' u'Teresa_Perales' u','\n",
      " u'que' u'logr\\xf3' u'la' u'plata' u'en' u'50' u'metros' u'mariposa'\n",
      " u'reconoci\\xf3' u'que' u'el' u'ambiente' u'\"' u'es' u'fant\\xe1stico' u','\n",
      " u'*0*' u'no' u'me' u'lo' u'esperaba' u'.']\n",
      "\n",
      "training_tags:4824\n",
      "train_tags: 3618\n",
      "test_tags: 1206\n",
      "eval_tags: 1206\n",
      "\n",
      "[u'sn.e-SUJ' u'rn' u'p0300000' u'vmip3p0' u'da0fp0' u'ncfp000' u'sps00'\n",
      " u'np0000a' u'cc' u'cs' u'np0000p' u'vmip3s0' u'di0mp0' u'da0mp0'\n",
      " u'ncmp000' u'sps00' u'dp3cs0' u'ncms000' u'Fc' u'cs' u'sn.e-SUJ'\n",
      " u'p0300000' u'vaip3s0' u'vmp00sm' u'aq0cs0' u'sps00' u'da0mp0' u'ao0mp0'\n",
      " u'ncmp000' u'Fc' u'rn' u'p0000000' u'vmip3s0' u'vmn0000' u'sps00'\n",
      " u'di0fs0' u'di0fs0' u'aq0cs0' u'ncfs000' u'aq0fs0' u'sps00' u'Z' u'cc'\n",
      " u'Z' u'cs' u'da0fs0' u'aq0fs0' u'Fp']\n",
      "[u'da0ms0' u'ncms000' u'pr0cn000' u'rg' u'vmip3s0' u'pi0cs000' u'pr0cn000'\n",
      " u'rn' u'p0300000' u'vmip3s0' u'sps00' u'di0ms0' u'ncms000' u'rg'\n",
      " u'aq0cs0' u'pr0cn000' u'pp3msa00' u'vmsp3s0' u'vmn0000' u'sps00'\n",
      " u'di0ms0' u'ncms000' u'aq0ms0' u'Fc' u'vmip3s0' u'vmg0000' u'da0fp0'\n",
      " u'ncfp000' u'sps00' u'ncfs000' u'Fp']\n",
      "[u'da0fs0' u'ncfs000' u'aq0fs0' u'aq0fs0' u'np0000p' u'Fc' u'pr0cn000'\n",
      " u'vmis3s0' u'da0fs0' u'ncfs000' u'sps00' u'Z' u'ncmp000' u'aq0cs0'\n",
      " u'vmis3s0' u'cs' u'da0ms0' u'ncms000' u'Fe' u'vsip3s0' u'aq0ms0' u'Fc'\n",
      " u'sn.e-SUJ' u'rn' u'p010s000' u'pp3msa00' u'vmii3s0' u'Fp']\n"
     ]
    }
   ],
   "source": [
    "print(\"training_sentences:\" + str(len(training_sentences)))\n",
    "print(\"train_sentences: \" + str(len(train_sentences)))\n",
    "print(\"test_sentences: \" + str(len(test_sentences)))\n",
    "print(\"eval_sentences: \" + str(len(eval_sentences)) + \"\\n\")\n",
    "\n",
    "print(train_sentences[0])\n",
    "print(test_sentences[0])\n",
    "print(eval_sentences[0])\n",
    "\n",
    "print(\"\\ntraining_tags:\" + str(len(training_sentences)))\n",
    "print(\"train_tags: \" + str(len(train_tags)))\n",
    "print(\"test_tags: \" + str(len(test_tags)))\n",
    "print(\"eval_tags: \" + str(len(eval_tags)) + \"\\n\")\n",
    "\n",
    "print(train_tags[0])\n",
    "print(test_tags[0])\n",
    "print(eval_tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
